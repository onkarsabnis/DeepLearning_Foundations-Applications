# DeepLearning_Foundations-Applications
*Deep Learning Foundations and Applications course(DLFA : AI61002)* under Center for Artificial Intelligence, IIT Kharagpur

## Train LeNet5 and Fully Connected NN on MNIST with SGD and Adam Optimizer and analyze:

• Effect of training loss and test loss vs. Batch size for a fixed learning rate   <br />
• Effect of training loss and test loss vs. Learning rate for a fixed Batch size    <br />
• Stochastic Gradient Descent with Restarts(SGDR) : A variant of learning rate annealing, which gradually decreases the learning rate through training   
